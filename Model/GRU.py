import pandas as pd
import tensorflow as tf
from sklearn.model_selection import train_test_split
import pickle
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.metrics import accuracy_score
from keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout, GRU
from tensorflow.keras.regularizers import L1L2, L1 , L2

if __name__ == '__main__':

    # Read the pickle file
    data = pd.read_pickle("Model/dataset.pickle")

    #Split the data into training and testing 
    x_train, xTest, y_train, yTest = train_test_split(data['Source Code'],data['Label'],test_size=0.2,random_state=1)
    
    #Split the training data into training and validation
    xTrain, xValidate, yTrain, yValidate = train_test_split(x_train,y_train,test_size=0.2,random_state=1)

    #Initialize tokenizer
    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=1000)
    tokenizer.fit_on_texts(list(data['Source Code']))

    #Convert training tokens into sequences
    train_tokenized = tokenizer.texts_to_sequences(xTrain)
    train_data = tf.keras.preprocessing.sequence.pad_sequences(train_tokenized,maxlen=300,padding='post')

    #Convert testing tokens into sequences
    test_tokenized = tokenizer.texts_to_sequences(xTest)
    test_data = tf.keras.preprocessing.sequence.pad_sequences(test_tokenized,maxlen=300,padding='post')
    
    #Convert validate tokens into seqeunces 
    validate_tokenized = tokenizer.texts_to_sequences(xValidate)
    validate_data = tf.keras.preprocessing.sequence.pad_sequences(validate_tokenized,maxlen=300,padding='post')

    non_vulnerable = len(data[data['Label']==0])
    vulnerable = len(data[data['Label']==1])
    total = non_vulnerable + vulnerable

    weight_for_0 = (1 / non_vulnerable)*(total)/2.0 
    weight_for_1 = (1 / vulnerable)*(total)/2.0

    class_weight = {0: weight_for_0, 1: weight_for_1}

    model= tf.keras.Sequential([
        Embedding(input_dim=1000, output_dim=13, input_length=300),
        GRU(13,return_sequences=True),
        Dropout(0.5),
        GRU(13),
        Dense(1,activation='sigmoid')
    ])

    METRICS = [
      tf.keras.metrics.TruePositives(name='tp'),
      tf.keras.metrics.FalsePositives(name='fp'),
      tf.keras.metrics.TrueNegatives(name='tn'),
      tf.keras.metrics.FalseNegatives(name='fn'), 
      tf.keras.metrics.BinaryAccuracy(name='accuracy'),
      tf.keras.metrics.Precision(name='precision'),
      tf.keras.metrics.Recall(name='recall'),
      tf.keras.metrics.AUC(name='auc'),
    ]

    #compile
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=METRICS)
    print(model.summary())

    # print(yTest)
    print(test_data[[1]])
    print(yTest[242])
    

    history = model.fit(train_data, yTrain ,batch_size=128, epochs = 20, validation_data=(validate_data, yValidate))
    y_pred = model.predict(test_data[[1]])
    print(y_pred)
    with open("Model/GRU_History",'wb') as history_file:
        pickle.dump(history.history,history_file)

    model.save("Model/GRU_Model")

    print(model.evaluate(test_data,yTest,batch_size=128))
